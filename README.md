<<<<<<< HEAD
# ForensicsDetective: PDF Source Classification

A machine learning project for digital forensics that classifies PDF documents based on their source application using metadata analysis. This system can identify whether a PDF was generated by Microsoft Word, Google Docs, LaTeX, Python/ReportLab, or HTML conversion with up to 96% accuracy.

---

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Project Structure](#project-structure)
- [Requirements](#requirements)
- [Installation](#installation)
- [Dataset Generation](#dataset-generation)
- [Usage](#usage)
- [Model Training and Evaluation](#model-training-and-evaluation)
- [Results](#results)
- [Team Members](#team-members)

---

## Overview

**ForensicsDetective** is a research project that demonstrates how PDF documents contain unique metadata signatures based on their creation method. By extracting and analyzing PDF metadata features (such as file size, page count, creation date, and structural attributes), machine learning classifiers can distinguish between documents created by different applications with exceptional accuracy.

This capability is critical in digital forensics for:
- Establishing document provenance
- Corroborating evidence
- Detecting potential document tampering
- Triaging large sets of PDF evidence

---

## Features

- **Multi-Source PDF Generation**: Automated scripts for generating PDFs from 5 different sources
- **Metadata-Based Classification**: Non-invasive analysis using only PDF metadata
- **Multiple ML Models**: Implementation of XGBoost, Random Forest, SVM, and SGD classifiers
- **High Accuracy**: Top models achieve 96% accuracy on 4-class classification
- **Scalable Pipeline**: Handles thousands of documents efficiently
- **Comprehensive Analysis**: Confusion matrices, performance metrics, and statistical evaluation

---

## Project Structure

```
ForensicsDetective/
├── README.md                          # Project overview and instructions
├── SETUP.md                           # Environment setup documentation
├── requirements.txt                   # Python dependencies
├── data/
│   ├── source_documents/              # Original source documents
│   ├── word_pdfs/                     # Microsoft Word PDFs
│   ├── google_docs_pdfs/              # Google Docs PDFs
│   ├── latex_pdfs/                    # LaTeX-generated PDFs
│   ├── python_pdfs/                   # Python/ReportLab PDFs
│   └── html_pdfs/                     # HTML-to-PDF conversions (bonus)
├── src/
│   ├── generate_word_pdfs.py          # Word PDF generation script
│   ├── google_docs_pdfs.py            # Google Docs PDF generation script
│   ├── latex_pdfs.py                  # LaTeX PDF generation script
│   ├── python_pdfs_generator.py       # Python/ReportLab PDF generation
│   ├── html_pdfs.py                   # HTML PDF generation script
│   ├── XGBoost_classifier.py          # XGBoost classifier implementation
│   ├── RandomForest_classifier.py     # Random Forest classifier
│   ├── SVM_classifier.py              # Support Vector Machine classifier
│   ├── SGD_classifier.py              # SGD classifier implementation
│   └── utils.py                       # Helper functions
├── results/
│   ├── confusion_matrices/            # Confusion matrix visualizations
│   ├── performance_metrics.csv        # Detailed performance results
│   └── statistical_analysis.py        # Statistical significance tests
└── reports/
    └── ForensicDetective_Research-Report.pdf  # Comprehensive findings report
```

---

## Requirements

### Software Dependencies

- Python 3.8 or higher
- Microsoft Word (for Word PDF generation)
- LaTeX distribution (TeX Live, MiKTeX, or MacTeX)
- Google Cloud account with Drive/Docs API enabled (for Google Docs generation)
- wkhtmltopdf (for HTML to PDF conversion)

### Python Libraries

All required Python packages are listed in `requirements.txt`. Key dependencies include:

```
numpy
pandas
scikit-learn
xgboost
PyPDF2
python-docx
reportlab
pdfkit
google-auth
google-auth-oauthlib
google-auth-httplib2
google-api-python-client
matplotlib
seaborn
```

---

## Installation

### Step 1: Clone the Repository

```bash
git clone https://github.com/Jeetkavaiya/ForensicDetective_Assignment.git
```

### Step 2: Set Up Python Environment

Create and activate a virtual environment:

```bash
# On Windows
python -m venv venv
venv\Scripts\activate

# On macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### Step 3: Install Python Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Install External Tools

**For LaTeX PDF generation:**
```bash
# Ubuntu/Debian
sudo apt-get install texlive-full

# macOS (using Homebrew)
brew install --cask mactex

# Windows: Download and install from https://miktex.org/
```

**For HTML to PDF conversion:**
```bash
# Ubuntu/Debian
sudo apt-get install wkhtmltopdf

# macOS
brew install wkhtmltopdf

# Windows: Download from https://wkhtmltopdf.org/downloads.html
```

### Step 5: Configure Google Cloud API (Optional)

If generating Google Docs PDFs:

1. Create a Google Cloud project at [console.cloud.google.com](https://console.cloud.google.com)
2. Enable Google Drive API and Google Docs API
3. Create a service account and download credentials JSON
4. Save credentials as `credentials.json` in the project root

---

## Dataset Generation

The project includes automated scripts to generate thousands of PDFs from different sources.

### Generate Microsoft Word PDFs

```bash
cd src
python generate_word_pdfs.py --count 5000 --output ../data/word_pdfs/
```

**Parameters:**
- `--count`: Number of PDFs to generate (default: 5000)
- `--output`: Output directory for generated PDFs

### Generate Google Docs PDFs

```bash
python google_docs_pdfs.py --count 5000 --credentials ../credentials.json --output ../data/google_docs_pdfs/
```

**Parameters:**
- `--count`: Number of PDFs to generate
- `--credentials`: Path to Google Cloud credentials JSON
- `--output`: Output directory

### Generate LaTeX PDFs

```bash
python latex_pdfs.py --count 5000 --output ../data/latex_pdfs/
```

### Generate Python/ReportLab PDFs

```bash
python python_pdfs_generator.py --count 5000 --output ../data/python_pdfs/
```

### Generate HTML PDFs (Bonus)

```bash
python html_pdfs.py --count 5000 --output ../data/html_pdfs/
```

---

## Usage

### Step 1: Prepare Your Dataset

Ensure you have generated or collected PDFs in the appropriate directories under `data/`.

### Step 2: Extract Metadata Features

The classifier scripts automatically extract metadata features from PDFs. Key features include:
- Number of pages
- File size
- File size per page
- Days since creation
- Compression attributes

### Step 3: Train and Evaluate Models

Run any of the classifier scripts:

```bash
cd src

# Train XGBoost classifier
python XGBoost_classifier.py

# Train Random Forest classifier
python RandomForest_classifier.py

# Train SVM classifier
python SVM_classifier.py

# Train SGD classifier
python SGD_classifier.py
```

Each script will:
1. Load and preprocess the PDF metadata
2. Split data into training and testing sets
3. Train the model with optimized hyperparameters
4. Evaluate performance on the test set
5. Generate confusion matrices and performance reports
6. Save results to the `results/` directory

---

## Model Training and Evaluation

### XGBoost Classifier

**Best Features:**
- Gradient boosting for high performance
- Handles complex non-linear relationships
- 96% accuracy on 4-class classification

**Usage:**
```bash
python XGBoost_classifier.py
```

### Random Forest Classifier

**Best Features:**
- Ensemble of decision trees
- Robust against overfitting
- 96% accuracy with balanced performance

**Usage:**
```bash
python RandomForest_classifier.py
```

### Support Vector Machine (SVM)

**Best Features:**
- RBF kernel for non-linear separation
- 92% accuracy
- Perfect classification of Google Docs

**Usage:**
```bash
python SVM_classifier.py
```

### Stochastic Gradient Descent (SGD)

**Best Features:**
- Fast and scalable linear classifier
- 89% accuracy
- Efficient for large datasets

**Usage:**
```bash
python SGD_classifier.py
```

---

## Results

### Performance Summary

| Classifier | Overall Accuracy | Key Strengths | Weaknesses |
|-----------|-----------------|---------------|------------|
| **XGBoost** | 96% | Balanced performance across all classes | Minimal |
| **Random Forest** | 96% | Excellent generalization | Minimal |
| **SVM** | 92% | Perfect Google Docs classification | Poor Python recall (0.70) |
| **SGD** | 89% | Fast training | High confusion between sources |

### Key Findings

1. **Google Docs Signature**: All models achieve 100% precision and recall for Google Docs PDFs, indicating a highly distinctive metadata signature.

2. **Ensemble Superiority**: Tree-based ensemble models (XGBoost and Random Forest) significantly outperform linear models.

3. **Challenging Distinctions**: The most common misclassifications occur between LaTeX, Python/ReportLab, and Word PDFs, which have more subtle metadata differences.

4. **Scalability Validated**: High accuracy maintained across thousands of documents with varying complexity (images, tables, multiple pages).

### Confusion Matrix Insights

The confusion matrices (available in `results/confusion_matrices/`) reveal:
- Strong diagonal values indicate correct classifications
- Off-diagonal clusters show systematic confusion patterns
- Google Docs forms a perfectly separated cluster

---

## Team Members

- **Jeet Kavaiya**
- **Gandhar Sidhaye**

---

## License

This project is developed for educational and research purposes as part of the 510 - Basics of AI course.

---

## Acknowledgments

- Original ForensicsDetective repository: [github.com/delveccj/ForensicsDetective](https://github.com/delveccj/ForensicsDetective)
- Course instructors: delveccj and AnushkaTi

---

## Contact

- For questions or collaboration inquiries, please contact the team members listed above.


